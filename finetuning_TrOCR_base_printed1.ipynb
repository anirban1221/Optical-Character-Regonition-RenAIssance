{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anirban1221/Optical-Character-Regonition-RenAIssance/blob/main/finetuning_TrOCR_base_printed1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In this file we will train the Finetune TrOCR-Base-Printed. We will first do text detection with Pytesseract then the output of those will be given to the model for training"
      ],
      "metadata": {
        "id": "-d53LU893RME"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PW0l3_rc3sK9",
        "outputId": "7490b297-6371-43e5-94ed-b7686faf12f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0W61HEA4Ya-",
        "outputId": "6d847326-4d12-456a-9093-2046ca7ff62b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.2.1)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n"
          ]
        }
      ],
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying image segmentation using pytessearact on the train images so that the words are detected. Then pairing the text-detected images with thir respective transcripts"
      ],
      "metadata": {
        "id": "rYaam63X1V4V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "arbfwfe54s0K",
        "outputId": "80eba643-f541-4384-94f9-3a13d6e44269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 86%|████████▌ | 12/14 [01:56<00:19,  9.70s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-deb7069d2cfe>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Use Tesseract for word-level detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytesseract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmorph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpytesseract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDICT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36mimage_to_data\u001b[0;34m(image, lang, config, nice, output_type, timeout, pandas_config)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m     return {\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0mOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBYTES\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         Output.DATAFRAME: lambda: get_pandas_output(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0mpandas_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         ),\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDICT\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfile_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_and_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0mOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTRING\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     }[output_type]()\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36mrun_and_get_output\u001b[0;34m(image, extension, lang, config, nice, timeout, return_bytes)\u001b[0m\n\u001b[1;32m    350\u001b[0m         }\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mrun_tesseract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         return _read_output(\n\u001b[1;32m    354\u001b[0m             \u001b[0;34mf\"{kwargs['output_filename_base']}{extsep}{extension}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36mrun_tesseract\u001b[0;34m(input_filename, output_filename_base, extension, lang, config, nice, timeout)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTesseractNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mtimeout_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror_string\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTesseractError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36mtimeout_manager\u001b[0;34m(proc, seconds)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m                 \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2113\u001b[0m                             'failed to raise TimeoutExpired.')\n\u001b[1;32m   2114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define paths\n",
        "input_dir = \"/content/drive/MyDrive/human_ai_folder/processed_images\"\n",
        "output_dir = \"/content/drive/MyDrive/human_ai_folder/detecting_words\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Get all image files\n",
        "image_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith('.jpg')]\n",
        "\n",
        "# Iterate through images\n",
        "for img_path in tqdm(image_files):\n",
        "    # Read image\n",
        "    image = cv2.imread(img_path)\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply adaptive thresholding\n",
        "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
        "\n",
        "    # Morphological operations to enhance text regions\n",
        "    kernel = np.ones((2, 2), np.uint8)\n",
        "    morph = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
        "    morph = cv2.dilate(morph, kernel, iterations=1)\n",
        "\n",
        "    # Use Tesseract for word-level detection\n",
        "    data = pytesseract.image_to_data(morph, output_type=pytesseract.Output.DICT)\n",
        "\n",
        "    for i in range(len(data['text'])):\n",
        "        if int(data['conf'][i]) > 30:  # Filter low-confidence words\n",
        "            x, y, w, h = data['left'][i], data['top'][i], data['width'][i], data['height'][i]\n",
        "            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "    # Save output image with bounding boxes\n",
        "    output_path = os.path.join(output_dir, os.path.basename(img_path))\n",
        "    cv2.imwrite(output_path, image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bC5xiWQm4z6D",
        "outputId": "31f3249d-7c8e-4f1f-afff-eb96a4acf72e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 14 image-text pairs\n",
            "Sample 1:\n",
            "  Image Path    : /content/drive/MyDrive/human_ai_folder/detecting_words/Buendia-page_1.jpg\n",
            "  Transcript Path: /content/drive/MyDrive/human_ai_folder/transcripts/transcript_Buendia/page1.txt\n",
            "--------------------------------------------------\n",
            "Sample 2:\n",
            "  Image Path    : /content/drive/MyDrive/human_ai_folder/detecting_words/Buendia-page_2.jpg\n",
            "  Transcript Path: /content/drive/MyDrive/human_ai_folder/transcripts/transcript_Buendia/page2.txt\n",
            "--------------------------------------------------\n",
            "Sample 3:\n",
            "  Image Path    : /content/drive/MyDrive/human_ai_folder/detecting_words/Buendia-page_3.jpg\n",
            "  Transcript Path: /content/drive/MyDrive/human_ai_folder/transcripts/transcript_Buendia/page3.txt\n",
            "--------------------------------------------------\n",
            "Sample 4:\n",
            "  Image Path    : /content/drive/MyDrive/human_ai_folder/detecting_words/Constituciones-page_1.jpg\n",
            "  Transcript Path: /content/drive/MyDrive/human_ai_folder/transcripts/transcript_Constituciones sinodales/page1.txt\n",
            "--------------------------------------------------\n",
            "Sample 5:\n",
            "  Image Path    : /content/drive/MyDrive/human_ai_folder/detecting_words/Constituciones-page_2.jpg\n",
            "  Transcript Path: /content/drive/MyDrive/human_ai_folder/transcripts/transcript_Constituciones sinodales/page2.txt\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from glob import glob\n",
        "\n",
        "# Define paths\n",
        "img_dir = \"/content/drive/MyDrive/human_ai_folder/detecting_words\"\n",
        "txt_dir = \"/content/drive/MyDrive/human_ai_folder/transcripts\"\n",
        "\n",
        "# Manual mapping for inconsistent names\n",
        "name_correction = {\n",
        "    \"Constituciones\": \"Constituciones sinodales\"\n",
        "}\n",
        "\n",
        "# Get all image file paths\n",
        "image_files = sorted(glob(os.path.join(img_dir, \"*.jpg\")))\n",
        "\n",
        "# Function to extract document name and page number from image filename\n",
        "def extract_doc_and_page(image_filename):\n",
        "    match = re.match(r\"(.*)-page_(\\d+)\", image_filename)\n",
        "    if match:\n",
        "        doc_name, page_num = match.groups()\n",
        "\n",
        "        # Correct document name if needed\n",
        "        doc_name = name_correction.get(doc_name, doc_name)\n",
        "\n",
        "        return doc_name, f\"page{page_num}.txt\"\n",
        "    return None, None\n",
        "\n",
        "# Create mapping between images and transcripts\n",
        "image_text_pairs = []\n",
        "for img_path in image_files:\n",
        "    img_name = os.path.basename(img_path).replace(\".jpg\", \"\")\n",
        "    doc_name, page_filename = extract_doc_and_page(img_name)\n",
        "\n",
        "    if doc_name and page_filename:\n",
        "        transcript_folder = os.path.join(txt_dir, f\"transcript_{doc_name}\")\n",
        "        text_path = os.path.join(transcript_folder, page_filename)\n",
        "\n",
        "        if os.path.exists(text_path):\n",
        "            image_text_pairs.append((img_path, text_path))\n",
        "\n",
        "# Check dataset size\n",
        "print(f\"Found {len(image_text_pairs)} image-text pairs\")\n",
        "\n",
        "# Display a few samples\n",
        "for i, (img, txt) in enumerate(image_text_pairs[:5]):  # Show only 5 samples\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"  Image Path    : {img}\")\n",
        "    print(f\"  Transcript Path: {txt}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Augmenting the images with a little bit of noise and rotation for generalising the data"
      ],
      "metadata": {
        "id": "lbbC2Wf72GfO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEF3RkXl5ah0"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define augmentation pipeline\n",
        "augmentations = A.Compose([\n",
        "    A.Rotate(limit=3, p=0.5),  # Slight rotation\n",
        "    A.RandomBrightnessContrast(p=0.5),  # Adjust brightness & contrast\n",
        "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.4),  # Inject noise\n",
        "    A.MotionBlur(blur_limit=3, p=0.3),  # Simulate motion blur\n",
        "])\n",
        "\n",
        "# Define output directory for augmented images\n",
        "augmented_data_dir = \"/content/drive/MyDrive/human_ai_folder/augmented_images\"\n",
        "os.makedirs(augmented_data_dir, exist_ok=True)\n",
        "\n",
        "num_augmentations = 10  # Generate 10 augmented versions per image\n",
        "augmented_pairs = []\n",
        "\n",
        "# Loop through original image-text pairs\n",
        "for img_path, text_path in tqdm(image_text_pairs):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Save the original image (unchanged) in augmented dataset\n",
        "    img_name = os.path.basename(img_path)\n",
        "    orig_img_path = os.path.join(augmented_data_dir, img_name)\n",
        "    cv2.imwrite(orig_img_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
        "    augmented_pairs.append((orig_img_path, text_path))\n",
        "\n",
        "    # Generate multiple augmented images per original\n",
        "    for i in range(1, num_augmentations + 1):\n",
        "        augmented_img = augmentations(image=img)[\"image\"]\n",
        "        aug_img_name = img_name.replace(\".jpg\", f\"_aug{i}.jpg\")\n",
        "\n",
        "        aug_img_path = os.path.join(augmented_data_dir, aug_img_name)\n",
        "        cv2.imwrite(aug_img_path, cv2.cvtColor(augmented_img, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        # Store mapping (augmented image -> original transcript)\n",
        "        augmented_pairs.append((aug_img_path, text_path))\n",
        "\n",
        "# Check dataset size\n",
        "print(f\"Total augmented dataset size: {len(augmented_pairs)} images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_OmamRL5gj3"
      },
      "outputs": [],
      "source": [
        "\n",
        "for i, (img, txt) in enumerate(augmented_pairs[:5]):\n",
        "    print(f\"Augmented Sample {i+1}:\")\n",
        "    print(f\"  Image Path    : {img}\")\n",
        "    print(f\"  Transcript Path: {txt}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1dMfJ3i5rgY"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch torchvision Pillow datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making the OCRdataset and importing the TrOCR baseline model to be finetuned"
      ],
      "metadata": {
        "id": "PTG_WbKt2mIj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkchOhX15rc9"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "class OCRDataset(Dataset):\n",
        "    def __init__(self, image_text_pairs, processor,max_length=512):\n",
        "        self.image_text_pairs = image_text_pairs\n",
        "        self.processor = processor\n",
        "        self.max_length=max_length\n",
        "\n",
        "    def normalize_text(self, text):\n",
        "        \"\"\"Apply historical OCR normalization rules\"\"\"\n",
        "        text = text.replace(\"ſ\", \"s\")  # Long 's' → 's'\n",
        "        text = text.replace(\"ç\", \"z\")  # ç → z\n",
        "        text = text.replace(\"q̄\", \"que\")  # q-macron → que\n",
        "        text = text.replace(\"u\", \"v\") if \"v\" in text else text.replace(\"v\", \"u\")  # u ↔ v\n",
        "        text = text.translate(str.maketrans(\"áéíóú\", \"aeiou\"))  # Remove accents (except ñ)\n",
        "        return text\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_text_pairs)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, text_path = self.image_text_pairs[idx]\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Load text\n",
        "        with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read().strip()\n",
        "\n",
        "        text = self.normalize_text(text)\n",
        "\n",
        "\n",
        "        # Process image and text\n",
        "        encoding = self.processor(\n",
        "            image,\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": encoding[\"pixel_values\"].squeeze(0),  # Image tensor\n",
        "            \"labels\": encoding[\"labels\"].squeeze(0),        # Tokenized text\n",
        "            }\n",
        "\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
        "\n",
        "    labels = pad_sequence([item['labels'] for item in batch], batch_first=True, padding_value=-100)\n",
        "    return {'pixel_values': pixel_values, 'labels': labels}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6aUEG043Nw2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uJ4qpzrlNwmp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPfbHPli5raY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "# Select a random image-text pair\n",
        "img_path, txt_path = random.choice(augmented_pairs)\n",
        "\n",
        "# Display the image\n",
        "img = Image.open(img_path)\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# Read and print corresponding text\n",
        "with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "print(f\"Extracted Text:\\n{text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYCmafy45rYj"
      },
      "outputs": [],
      "source": [
        "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
        "\n",
        "# Load the processor and model\n",
        "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-printed\")\n",
        "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\")\n",
        "\n",
        "# Ensure model is on GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "print(device)\n",
        "# Check model structure\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKGpv6CW5rVs"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split dataset into train and validation sets (80% train, 20% validation)\n",
        "train_pairs, val_pairs = train_test_split(augmented_pairs, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create dataset objects\n",
        "train_dataset = OCRDataset(train_pairs, processor)\n",
        "val_dataset = OCRDataset(val_pairs, processor)\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f\"Training Set Size: {len(train_dataset)}\")\n",
        "print(f\"Validation Set Size: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWCZCsaNgFhy"
      },
      "outputs": [],
      "source": [
        "def postprocess_ocr_output(text):\n",
        "    text = text.replace(\"ſ\", \"s\")  # Long 's' → 's'\n",
        "    text = text.replace(\"ç\", \"z\")  # ç → z\n",
        "    text = text.replace(\"q̄\", \"que\")  # q-macron → que\n",
        "    text = text.translate(str.maketrans(\"áéíóú\", \"aeiou\"))  # Remove accents (except ñ)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxsgGKzu6F69"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import Trainer\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def __init__(self, processor,*args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.ctc_loss = nn.CTCLoss(blank=processor.tokenizer.pad_token_id, zero_infinity=True)\n",
        "        self.processor=processor\n",
        "\n",
        "    def compute_loss(self, model, inputs,num_items_in_batch=None, return_outputs=False):\n",
        "        pixel_values = inputs[\"pixel_values\"]\n",
        "        labels = inputs[\"labels\"]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(pixel_values, labels=labels)\n",
        "        logits = outputs.logits  # Model predictions        loss = outputs.loss  # Default loss\n",
        "\n",
        "        # Computed the CTC loss manually\n",
        "        input_lengths = torch.full(\n",
        "            (logits.shape[0],), logits.shape[1], dtype=torch.long\n",
        "        )  # Assume full sequence length\n",
        "        target_lengths = (labels != -100).sum(dim=-1)  # Counting of non-padding tokens\n",
        "\n",
        "        # Replace -100 with pad_token_id before computing loss\n",
        "        labels = torch.where(labels == -100, processor.tokenizer.pad_token_id, labels)\n",
        "\n",
        "        loss = self.ctc_loss(\n",
        "            logits.permute(1, 0, 2).log_softmax(2),  # (seq_len, batch, vocab_size)\n",
        "            labels,\n",
        "            input_lengths,\n",
        "            target_lengths\n",
        "        )\n",
        "\n",
        "        pred_texts = self.processor.batch_decode(logits.argmax(dim=-1), skip_special_tokens=True)\n",
        "        gt_texts = self.processor.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        pred_texts = [postprocess_ocr_output(txt) for txt in pred_texts]  # Apply postprocessing\n",
        "        gt_texts = [postprocess_ocr_output(txt) for txt in gt_texts]\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EycVPvkjKO86"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFfgajCyKO5C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cv-xY2Vq6HYy"
      },
      "outputs": [],
      "source": [
        "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
        "model.config.pad_token_id = processor.tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmZB-1Wc6HKP"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/human_ai_folder/ocr_model\",\n",
        "    per_device_train_batch_size=4,  # Reduce if still OOM\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=2,  # Accumulate gradients to simulate larger batch size\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=4,\n",
        "    num_train_epochs=15,\n",
        "    logging_dir=\"/content/drive/MyDrive/human_ai_folder/log\",\n",
        "    logging_steps=50,\n",
        "    logging_strategy=\"steps\",\n",
        "    learning_rate=3e-5,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=500,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=True,\n",
        "    push_to_hub=False\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    data_collator=collate_fn,\n",
        "    processor=processor\n",
        "     # Pass custom loss function\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwqOWHzJ6SVx"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "# Reset and reinitialize the accelerator\n",
        "accelerator = Accelerator()\n",
        "accelerator.state._reset_state()\n",
        "accelerator = Accelerator()  # Reinitialize\n",
        "\n",
        "# Ensure Trainer is using the new accelerator\n",
        "trainer.accelerator = accelerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3I-dRQaN6T_L"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sample testing for model evaluation"
      ],
      "metadata": {
        "id": "rqaZWbXf3Kwj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5va5xYTZd1fJ"
      },
      "outputs": [],
      "source": [
        "image_path = \"/content/drive/MyDrive/human_ai_folder/testing_folder/Mendo-page_4.jpg\"\n",
        "image = cv2.imread(image_path)\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZHJUAjseh6B"
      },
      "outputs": [],
      "source": [
        "data = pytesseract.image_to_data(gray, output_type=pytesseract.Output.DICT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QO7nfhL6e4Ci"
      },
      "outputs": [],
      "source": [
        "for i in range(len(data['text'])):\n",
        "    if int(data['conf'][i]) > 0:  # Ignore low-confidence detections\n",
        "        x, y, w, h = data['left'][i], data['top'][i], data['width'][i], data['height'][i]\n",
        "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "# Show the image with bounding boxes\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkITGe79E0vv"
      },
      "outputs": [],
      "source": [
        "from transformers import VisionEncoderDecoderModel, TrOCRProcessor\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# Specify the path to the latest/best checkpoint\n",
        "checkpoint_path = \"/content/drive/MyDrive/human_ai_folder/ocr_model/checkpoint-124\"  # Change this to the latest checkpoint\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model = VisionEncoderDecoderModel.from_pretrained(checkpoint_path)\n",
        "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-printed\")\n",
        "\n",
        "# Ensure model is in evaluation mode\n",
        "model.eval()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyITOm-6PUs5"
      },
      "outputs": [],
      "source": [
        "# Function to perform OCR on an image\n",
        "def process_text_regions(image, data):\n",
        "    extracted_text = \"\"\n",
        "\n",
        "    for i in range(len(data['text'])):\n",
        "        text = data['text'][i].strip()\n",
        "        if text:\n",
        "            x, y, w, h = data['left'][i], data['top'][i], data['width'][i], data['height'][i]\n",
        "            cropped_region = image[y:y+h, x:x+w]\n",
        "            pil_img = Image.fromarray(cropped_region).convert(\"RGB\")\n",
        "\n",
        "            # Process with TrOCR\n",
        "            pixel_values = processor(pil_img, return_tensors=\"pt\").pixel_values\n",
        "            generated_ids = model.generate(pixel_values)\n",
        "            decoded_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "            extracted_text += decoded_text + \" \"\n",
        "\n",
        "    return extracted_text\n",
        "\n",
        "# Extract and recognize text\n",
        "final_text = process_text_regions(gray, data)\n",
        "print(\"Extracted Text:\")\n",
        "print(final_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tO42Sx1Wdcdu"
      },
      "outputs": [],
      "source": [
        "    def normalize_text(text):\n",
        "        \"\"\"Apply historical OCR normalization rules\"\"\"\n",
        "        text = text.replace(\"ſ\", \"s\")  # Long 's' → 's'\n",
        "        text = text.replace(\"ç\", \"z\")  # ç → z\n",
        "        text = text.replace(\"q̄\", \"que\")  # q-macron → que\n",
        "        text = text.replace(\"u\", \"v\") if \"v\" in text else text.replace(\"v\", \"u\")  # u ↔ v\n",
        "        text = text.translate(str.maketrans(\"áéíóú\", \"aeiou\"))  # Remove accents (except ñ)\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0DuyAhNg4Uv"
      },
      "outputs": [],
      "source": [
        "transcript_path = \"/content/drive/MyDrive/human_ai_folder/testing_folder/page4.txt\"\n",
        "with open(transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    ground_truth_text = f.read().strip()\n",
        "\n",
        "\n",
        "ground_truth_text=normalize_text(ground_truth_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRS_R-sKoih5"
      },
      "outputs": [],
      "source": [
        "print(\"\\nGround Truth Transcript:\\n\", ground_truth_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWhqYnWxtN50"
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnF6Uufuolgp"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import spacy\n",
        "\n",
        "# Load Spanish NLP model for sentence and proper noun detection\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "def correct_case(text):\n",
        "    # Convert everything to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Process text using spaCy for sentence detection\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Capitalize the first letter of each sentence\n",
        "    corrected_text = \" \".join(\n",
        "        [sent.text.capitalize() for sent in doc.sents]\n",
        "    )\n",
        "\n",
        "    return corrected_text\n",
        "\n",
        "# Example usage\n",
        "extracted_text = final_text  # Your OCR output\n",
        "clean_text = correct_case(extracted_text)\n",
        "print(clean_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G31qDWiDtf-L"
      },
      "outputs": [],
      "source": [
        "!pip install Levenshtein jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvhbA8wytCDJ"
      },
      "outputs": [],
      "source": [
        "import pytesseract\n",
        "import cv2\n",
        "import Levenshtein\n",
        "from jiwer import wer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpSDntnutgou"
      },
      "outputs": [],
      "source": [
        "cer = Levenshtein.distance(clean_text, ground_truth_text) / max(len(ground_truth_text), 1)\n",
        "wer_value = wer(ground_truth_text, clean_text)\n",
        "print(f\"\\nCER (Character Error Rate): {cer:.4f}\")\n",
        "print(f\"WER (Word Error Rate): {wer_value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R9xK6XJVjYH1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOpSDsUAFwRbHMe/F0ezUPN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}